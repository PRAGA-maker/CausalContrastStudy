{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eaa6a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- CELL: IMPUTATION AUDIT & MODEL CHALLENGE (FAST VERSION) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 1. Install XGBoost if missing\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    print(\"Installing XGBoost...\")\n",
    "    !{sys.executable} -m pip install xgboost\n",
    "    import xgboost as xgb\n",
    "\n",
    "print(\"\\n=== 1. DATA AUDIT: SCALE OF IMPUTATION ===\")\n",
    "raw_cols = ['baseline_egfr', 'baseline_creat']\n",
    "source_df = df_cohort_aligned if 'baseline_egfr' in df_cohort_aligned.columns else df_cohort.loc[df_cohort_aligned.index]\n",
    "\n",
    "n_total = len(source_df)\n",
    "for col in raw_cols:\n",
    "    n_missing = source_df[col].isna().sum()\n",
    "    pct_missing = (n_missing / n_total) * 100\n",
    "    print(f\"Feature '{col}': {n_missing:,} missing values ({pct_missing:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== 2. MATRIX CONSTRUCTION ===\")\n",
    "print(\"Building Challenger Matrix (Raw Data + Sparse Codes)...\")\n",
    "\n",
    "# A. Current Matrix (Imputed + One-Hot) -> Already exists as X_all\n",
    "print(f\"Baseline Matrix (X_all): {X_all.shape}\")\n",
    "\n",
    "# B. Challenger Matrix (Raw Floats + Sparse Codes)\n",
    "# 1. Grab Raw Dense Data & Force Numeric Types\n",
    "dense_raw = source_df[['age', 'baseline_egfr', 'baseline_creat', 'site_contrast_rate']].copy()\n",
    "dense_raw = dense_raw.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 2. Convert to Sparse Matrix (CSR)\n",
    "# FIX: Explicitly cast to float to prevent the 'dtype(O)' error\n",
    "X_dense_raw = sp.csr_matrix(dense_raw.values.astype(float))\n",
    "\n",
    "# 3. Stack with the existing Sparse Codes\n",
    "X_challenger = sp.hstack([X_dense_raw, X_sparse], format='csr')\n",
    "\n",
    "print(f\"Challenger Matrix (X_challenger): {X_challenger.shape} (Raw NaNs, No Manual Bins)\")\n",
    "\n",
    "# === 3. HEAD-TO-HEAD BATTLE ===\n",
    "print(\"\\n=== 3. MODEL PERFORMANCE TEST ===\")\n",
    "\n",
    "y = df_cohort_aligned['contrast_received'].values\n",
    "X_train_base, X_test_base, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_chal, X_test_chal, _, _ = train_test_split(X_challenger, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- MODEL A: BASELINE (Lasso) ---\n",
    "# SPEED FIX: solver='liblinear' and tol=0.01 for faster convergence check\n",
    "print(\"\\nTraining Baseline: Logistic Regression (Lasso)...\")\n",
    "t0 = time.time()\n",
    "model_base = LogisticRegression(\n",
    "    penalty='l1', \n",
    "    solver='liblinear',  # Faster than 'saga' for single-core binary\n",
    "    tol=0.01,            # Relax tolerance for speed (default is 0.0001)\n",
    "    C=0.1, \n",
    "    class_weight='balanced', \n",
    "    max_iter=1000\n",
    ")\n",
    "model_base.fit(X_train_base, y_train)\n",
    "preds_base = model_base.predict_proba(X_test_base)[:, 1]\n",
    "time_base = time.time() - t0\n",
    "auc_base = roc_auc_score(y_test, preds_base)\n",
    "\n",
    "# --- MODEL B: CHALLENGER (XGBoost) ---\n",
    "print(\"Training Challenger: XGBoost (Native Missing Handling)...\")\n",
    "t0 = time.time()\n",
    "model_chal = xgb.XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    tree_method='hist', # Optimized for speed\n",
    "    n_jobs=-1,          # XGBoost CAN multiprocess!\n",
    "    missing=np.nan,     # Explicitly tell it to treat NaNs as missing\n",
    "    scale_pos_weight=(len(y_train)-y_train.sum())/y_train.sum() \n",
    ")\n",
    "model_chal.fit(X_train_chal, y_train)\n",
    "preds_chal = model_chal.predict_proba(X_test_chal)[:, 1]\n",
    "time_chal = time.time() - t0\n",
    "auc_chal = roc_auc_score(y_test, preds_chal)\n",
    "\n",
    "print(\"\\n=== 4. RESULTS REPORT ===\")\n",
    "print(f\"Baseline AUC (Imputed + Lasso):  {auc_base:.4f} (Time: {time_base:.1f}s)\")\n",
    "print(f\"Challenger AUC (Raw + XGBoost):  {auc_chal:.4f} (Time: {time_chal:.1f}s)\")\n",
    "\n",
    "if auc_chal >= auc_base - 0.01:\n",
    "    print(\"\\nSUCCESS: XGBoost matches or beats Lasso using RAW data.\")\n",
    "    print(\"Action: You can safely delete the imputation code in Cell 6.\")\n",
    "else:\n",
    "    print(\"\\nCAUTION: Lasso performed significantly better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad3f04",
   "metadata": {},
   "source": [
    "result: \n",
    "\n",
    "\n",
    "=== 1. DATA AUDIT: SCALE OF IMPUTATION ===\n",
    "Feature 'baseline_egfr': 47,285 missing values (75.0%)\n",
    "Feature 'baseline_creat': 13,346 missing values (21.2%)\n",
    "\n",
    "=== 2. MATRIX CONSTRUCTION ===\n",
    "Building Challenger Matrix (Raw Data + Sparse Codes)...\n",
    "Baseline Matrix (X_all): (63064, 69542)\n",
    "Challenger Matrix (X_challenger): (63064, 69516) (Raw NaNs, No Manual Bins)\n",
    "\n",
    "=== 3. MODEL PERFORMANCE TEST ===\n",
    "\n",
    "Training Baseline: Logistic Regression (Lasso)...\n",
    "Training Challenger: XGBoost (Native Missing Handling)...\n",
    "\n",
    "=== 4. RESULTS REPORT ===\n",
    "Baseline AUC (Imputed + Lasso):  0.7627 (Time: 3.2s)\n",
    "Challenger AUC (Raw + XGBoost):  0.7854 (Time: 35.5s)\n",
    "\n",
    "SUCCESS: XGBoost matches or beats Lasso using RAW data.\n",
    "Action: You can safely delete the imputation code in Cell 6."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
