{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31deaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, site\n",
    "\n",
    "# Make sure the user site-packages dir (/home/jupyter/.local/...) is on sys.path\n",
    "try:\n",
    "    user_site = site.getusersitepackages()\n",
    "    if user_site not in sys.path:\n",
    "        sys.path.append(user_site)\n",
    "        print(\"Added user site-packages to sys.path:\", user_site)\n",
    "    else:\n",
    "        print(\"User site-packages already on sys.path:\", user_site)\n",
    "except Exception as e:\n",
    "    print(\"Could not resolve user site-packages:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: SETUP ---\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scipy matplotlib seaborn tqdm scikit-learn lifelines pandas-gbq google-cloud-bigquery fastparquet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import subprocess\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# --- Machine Learning & Stats ---\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import fastparquet\n",
    "\n",
    "# --- Survival Analysis ---\n",
    "import sys, subprocess, site\n",
    "\n",
    "# ensure user site is on path **before** importing\n",
    "try:\n",
    "    user_site = site.getusersitepackages()\n",
    "    if user_site not in sys.path:\n",
    "        sys.path.append(user_site)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "    from lifelines.statistics import logrank_test\n",
    "except ImportError:\n",
    "    # install into the current interpreter's env\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lifelines\"])\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "# --- Configurations ---\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "WORKSPACE_CDR = os.environ.get(\"WORKSPACE_CDR\", \"\")\n",
    "N_CORES = max(1, os.cpu_count() - 2)\n",
    "\n",
    "print(f\"Environment Ready. CDR: {WORKSPACE_CDR}, Cores: {N_CORES}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def to_naive_utc_day(series):\n",
    "    \"\"\"Robustly converts mixed timezones to naive UTC midnight.\"\"\"\n",
    "    return pd.to_datetime(series, errors='coerce', utc=True).dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "def clean_mem():\n",
    "    \"\"\"Forces garbage collection.\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "def calculate_ess(weights):\n",
    "    \"\"\"Calculates Kish's Effective Sample Size.\"\"\"\n",
    "    if len(weights) == 0: return 0\n",
    "    return (np.sum(weights) ** 2) / np.sum(weights ** 2)\n",
    "\n",
    "def sparse_weighted_mean_var(X, weights):\n",
    "    \"\"\"Calculates means/vars of sparse matrix X with weights without densifying.\"\"\"\n",
    "    # X is (N, P), weights is (N,)\n",
    "    W = sp.diags(weights)\n",
    "    X_weighted = W @ X\n",
    "    sum_w = np.sum(weights)\n",
    "    means = np.array(X_weighted.sum(axis=0) / sum_w).flatten()\n",
    "    \n",
    "    # Variance is trickier, simplified approximation for Love Plot speed:\n",
    "    # Var = E[X^2] - (E[X])^2\n",
    "    X2 = X.power(2)\n",
    "    means2 = np.array((W @ X2).sum(axis=0) / sum_w).flatten()\n",
    "    vars_ = means2 - (means ** 2)\n",
    "    return means, vars_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Clinical Definitions & Negative Controls (Refined)\n",
    "# =============================================================================\n",
    "print(\"\\n--- CELL 2: Clinical Definitions & Negative Controls ---\")\n",
    "\n",
    "# --- 1. EXPOSURES ---\n",
    "CONTRAST_CT = {4139745, 21492176, 4335400, 3047782, 4327032, 3013610, 36713226, 3053128, 4252907, 3019625}\n",
    "CONTRAST_MRI = {4335399, 4161393, 4202274, 4197203, 36717294, 45765683, 37117806, 37109194, 37109196}\n",
    "CONTRAST_IDS = CONTRAST_CT.union(CONTRAST_MRI)\n",
    "\n",
    "NON_CONTRAST_CT = {37109313, 3049940, 37117305, 3047921, 36713200, 3018999, 40771605, 36713202, 3035568}\n",
    "NON_CONTRAST_MRI = {37109312, 36713204, 36713045, 36713262, 3024397, 36713243, 3053040, 37109329, 42535581, 42535582}\n",
    "NON_CONTRAST_IDS = NON_CONTRAST_CT.union(NON_CONTRAST_MRI)\n",
    "\n",
    "ALL_IMAGING = CONTRAST_IDS.union(NON_CONTRAST_IDS)\n",
    "\n",
    "# --- 2. MAIN OUTCOMES (The Targets) ---\n",
    "# Naming Convention: Key is used for column naming (e.g., date_AKI_30)\n",
    "ANALYSIS_OUTCOMES = {\n",
    "    'AKI_30': ({761083, 197320, 40481064, 4328366, 37116432, 45757442, 37016366}, 30),\n",
    "    'NEW_DIALYSIS_90': ({4032243, 4146536, 4324124, 4019967, 40482357}, 90),\n",
    "    'MORTALITY_30': ('DEATH', 30),\n",
    "    'MAE_30': ('COMPOSITE', 30), # Defined as min(AKI, Dialysis, Death)\n",
    "    'THYROID_90': ({138384, 37016342, 45757058, 4032331}, 90)\n",
    "}\n",
    "\n",
    "# --- 3. COVARIATE DEFINITIONS (Not Policies) ---\n",
    "# Used for confounding adjustment, NOT for \"withholding\" rules\n",
    "THYROTOXICOSIS_IDS = {37016342, 45757058, 440936, 134438} \n",
    "\n",
    "# Lab Concepts - STRICT SEPARATION\n",
    "# Only use true eGFR codes for eGFR. Creatinine is a separate covariate.\n",
    "\n",
    "# eGFR (CKD-EPI, MDRD, etc.) â€“ ONLY true eGFR concepts\n",
    "EGFR_CONCEPTS = {\n",
    "    333096, 3049187, 3053283, 3029859, 1619026, 1619025\n",
    "}\n",
    "\n",
    "# Serum Creatinine\n",
    "CREATININE_CONCEPTS = {3016723, 3020564, 3034485, 3022192}\n",
    "\n",
    "\n",
    "# --- 4. NEGATIVE CONTROLS (The Calibrators) ---\n",
    "NEGATIVE_CONTROLS = {\n",
    "    'NC_Ingrown_Nail': {139900},\n",
    "    'NC_Ankle_Sprain': {4196156},\n",
    "    'NC_Cataract': {375545},\n",
    "    'NC_Otitis_Media': {378534},\n",
    "    'NC_T2DM': {201826}, \n",
    "    'NC_Hypertension': {320128},\n",
    "    'NC_Hyperlipidemia': {432867},\n",
    "    'NC_Gout': {439392},\n",
    "    'NC_Depression': {4282316}, \n",
    "    'NC_Anxiety': {436073}, \n",
    "    'NC_Insomnia': {436962},\n",
    "    'NC_Osteoarthritis': {4079750, 4155298},\n",
    "    'NC_Low_Back_Pain': {4213162},\n",
    "    'NC_Carpal_Tunnel': {376918},\n",
    "    'NC_Allergic_Rhinitis': {379805},\n",
    "    'NC_GERD': {192279}, \n",
    "    'NC_Migraine': {375527}, \n",
    "    'NC_Hypothyroidism': {140673},\n",
    "    'NC_Varicose_Veins': {318800}\n",
    "}\n",
    "\n",
    "ALL_NEGATIVE_CONTROL_IDS = set().union(*NEGATIVE_CONTROLS.values())\n",
    "\n",
    "# --- 5. EXCLUSION LIST (Covariates to drop) ---\n",
    "# We ONLY exclude concepts if they are the outcome itself occurring *after* index.\n",
    "# We do NOT exclude pre-index history of these conditions (they are confounders).\n",
    "# Specific logic applied in Cell 5.\n",
    "OUTCOME_CONCEPTS = set().union(*[v[0] for k,v in ANALYSIS_OUTCOMES.items() if isinstance(v[0], set)])\n",
    "\n",
    "print(f\"Definitions Loaded. {len(NEGATIVE_CONTROLS)} Negative Controls defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2.5: Empirical Null Calibration Engine\n",
    "# =============================================================================\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calibrate_estimates(results_df):\n",
    "    \"\"\"\n",
    "    Fits an Empirical Null distribution to Negative Controls and calibrates\n",
    "    the P-values and CIs for the Main Outcomes.\n",
    "    \n",
    "    Assumption: Negative Controls have True Log-RR = 0.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Performing Empirical Calibration ---\")\n",
    "    \n",
    "    # 1. Identify Negative Controls\n",
    "    # Assumes results_df has a column 'Type' or 'Outcome' starting with 'NC_'\n",
    "    nc_df = results_df[results_df['Outcome'].str.startswith('NC_')].copy()\n",
    "    \n",
    "    if len(nc_df) < 10:\n",
    "        print(\"WARNING: Too few negative controls (<10) for robust calibration.\")\n",
    "        return results_df\n",
    "    \n",
    "    # 2. Extract Log-RR and Standard Error (from CI)\n",
    "    # We use Log-RR because it's symmetric. \n",
    "    # SE = (Log(Upper) - Log(Lower)) / 3.92\n",
    "    nc_df['log_rr'] = np.log(nc_df['HR_Cox'].astype(float))\n",
    "    nc_df['se_log_rr'] = (np.log(nc_df['HR_CI_High'].astype(float)) - np.log(nc_df['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    # Drop invalid results (infinite or NaN)\n",
    "    nc_df = nc_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['log_rr', 'se_log_rr'])\n",
    "    \n",
    "    # 3. Fit the Null Distribution N(mu, sigma^2)\n",
    "    # We use a weighted moment estimator (inverse variance weighting)\n",
    "    weights = 1.0 / (nc_df['se_log_rr'] ** 2)\n",
    "    \n",
    "    # Mean bias (Systematic Shift)\n",
    "    null_mean = np.average(nc_df['log_rr'], weights=weights)\n",
    "    \n",
    "    # SD bias (Unmeasured Confounding width)\n",
    "    # Variance = weighted average of (x - mean)^2 - average sampling variance\n",
    "    raw_var = np.average((nc_df['log_rr'] - null_mean)**2, weights=weights)\n",
    "    expected_sampling_var = np.average(nc_df['se_log_rr']**2, weights=weights)\n",
    "    \n",
    "    # The systematic variance is the excess variance observed\n",
    "    null_var = max(0, raw_var - expected_sampling_var)\n",
    "    null_sd = np.sqrt(null_var)\n",
    "    \n",
    "    print(f\"  Empirical Null Fitted: Mean Bias = {null_mean:.4f}, SD Bias = {null_sd:.4f}\")\n",
    "    print(f\"  (Interpretation: Mean!=0 implies systematic error; SD>0 implies unmeasured confounding)\")\n",
    "    \n",
    "    # 4. Calibrate All Estimates (Main + NCs)\n",
    "    calibrated_results = results_df.copy()\n",
    "    \n",
    "    # Calculate Log stats for all rows\n",
    "    log_rr = np.log(calibrated_results['HR_Cox'].astype(float))\n",
    "    se_log_rr = (np.log(calibrated_results['HR_CI_High'].astype(float)) - np.log(calibrated_results['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    # Calibrated Z-Score\n",
    "    # We subtract the mean bias and divide by the wider uncertainty (sampling + systematic)\n",
    "    z_cal = (log_rr - null_mean) / np.sqrt(se_log_rr**2 + null_sd**2)\n",
    "    \n",
    "    # Calibrated P-value\n",
    "    calibrated_results['P_Calibrated'] = 2 * (1 - norm.cdf(np.abs(z_cal)))\n",
    "    \n",
    "    # Calibrated CIs (Shifted and Widened)\n",
    "    calibrated_se = np.sqrt(se_log_rr**2 + null_sd**2)\n",
    "    calibrated_results['HR_Calibrated'] = np.exp(log_rr - null_mean)\n",
    "    calibrated_results['HR_Cal_Low'] = np.exp((log_rr - null_mean) - 1.96 * calibrated_se)\n",
    "    calibrated_results['HR_Cal_High'] = np.exp((log_rr - null_mean) + 1.96 * calibrated_se)\n",
    "    \n",
    "    return calibrated_results\n",
    "\n",
    "# Function to plot the calibration (Funnel Plot)\n",
    "def plot_calibration(results_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot Negative Controls\n",
    "    ncs = results_df[results_df['Outcome'].str.startswith('NC_')]\n",
    "    log_rr_nc = np.log(ncs['HR_Cox'].astype(float))\n",
    "    se_nc = (np.log(ncs['HR_CI_High'].astype(float)) - np.log(ncs['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    plt.scatter(log_rr_nc, 1/se_nc, alpha=0.5, color='gray', label='Negative Controls')\n",
    "    \n",
    "    # Plot Main Outcomes\n",
    "    main = results_df[~results_df['Outcome'].str.startswith('NC_')]\n",
    "    log_rr_main = np.log(main['HR_Cox'].astype(float))\n",
    "    se_main = (np.log(main['HR_CI_High'].astype(float)) - np.log(main['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    plt.scatter(log_rr_main, 1/se_main, color='red', s=100, label='Main Outcomes', zorder=10)\n",
    "    \n",
    "    # Plot Null Line (x=0 is HR=1)\n",
    "    plt.axvline(0, color='black', linestyle='--')\n",
    "    \n",
    "    # Plot Fitted Null Area (Mean +/- SD)\n",
    "    # We just draw the region around 0 to show visual calibration\n",
    "    # (If using the fitted values, draw vertical lines at null_mean)\n",
    "    \n",
    "    plt.xlabel(\"Log Hazard Ratio\")\n",
    "    plt.ylabel(\"Precision (1/SE)\")\n",
    "    plt.title(\"Empirical Calibration Funnel Plot\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2.5: Empirical Null Calibration Engine\n",
    "# =============================================================================\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calibrate_estimates(results_df):\n",
    "    \"\"\"\n",
    "    Fits an Empirical Null distribution to Negative Controls and calibrates\n",
    "    the P-values and CIs for the Main Outcomes.\n",
    "    \n",
    "    Assumption: Negative Controls have True Log-RR = 0.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Performing Empirical Calibration ---\")\n",
    "    \n",
    "    # 1. Identify Negative Controls\n",
    "    # Assumes results_df has a column 'Type' or 'Outcome' starting with 'NC_'\n",
    "    nc_df = results_df[results_df['Outcome'].str.startswith('NC_')].copy()\n",
    "    \n",
    "    if len(nc_df) < 10:\n",
    "        print(\"WARNING: Too few negative controls (<10) for robust calibration.\")\n",
    "        return results_df\n",
    "    \n",
    "    # 2. Extract Log-RR and Standard Error (from CI)\n",
    "    # We use Log-RR because it's symmetric. \n",
    "    # SE = (Log(Upper) - Log(Lower)) / 3.92\n",
    "    nc_df['log_rr'] = np.log(nc_df['HR_Cox'].astype(float))\n",
    "    nc_df['se_log_rr'] = (np.log(nc_df['HR_CI_High'].astype(float)) - np.log(nc_df['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    # Drop invalid results (infinite or NaN)\n",
    "    nc_df = nc_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['log_rr', 'se_log_rr'])\n",
    "    \n",
    "    # 3. Fit the Null Distribution N(mu, sigma^2)\n",
    "    # We use a weighted moment estimator (inverse variance weighting)\n",
    "    weights = 1.0 / (nc_df['se_log_rr'] ** 2)\n",
    "    \n",
    "    # Mean bias (Systematic Shift)\n",
    "    null_mean = np.average(nc_df['log_rr'], weights=weights)\n",
    "    \n",
    "    # SD bias (Unmeasured Confounding width)\n",
    "    # Variance = weighted average of (x - mean)^2 - average sampling variance\n",
    "    raw_var = np.average((nc_df['log_rr'] - null_mean)**2, weights=weights)\n",
    "    expected_sampling_var = np.average(nc_df['se_log_rr']**2, weights=weights)\n",
    "    \n",
    "    # The systematic variance is the excess variance observed\n",
    "    null_var = max(0, raw_var - expected_sampling_var)\n",
    "    null_sd = np.sqrt(null_var)\n",
    "    \n",
    "    print(f\"  Empirical Null Fitted: Mean Bias = {null_mean:.4f}, SD Bias = {null_sd:.4f}\")\n",
    "    print(f\"  (Interpretation: Mean!=0 implies systematic error; SD>0 implies unmeasured confounding)\")\n",
    "    \n",
    "    # 4. Calibrate All Estimates (Main + NCs)\n",
    "    calibrated_results = results_df.copy()\n",
    "    \n",
    "    # Calculate Log stats for all rows\n",
    "    log_rr = np.log(calibrated_results['HR_Cox'].astype(float))\n",
    "    se_log_rr = (np.log(calibrated_results['HR_CI_High'].astype(float)) - np.log(calibrated_results['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    # Calibrated Z-Score\n",
    "    # We subtract the mean bias and divide by the wider uncertainty (sampling + systematic)\n",
    "    z_cal = (log_rr - null_mean) / np.sqrt(se_log_rr**2 + null_sd**2)\n",
    "    \n",
    "    # Calibrated P-value\n",
    "    calibrated_results['P_Calibrated'] = 2 * (1 - norm.cdf(np.abs(z_cal)))\n",
    "    \n",
    "    # Calibrated CIs (Shifted and Widened)\n",
    "    calibrated_se = np.sqrt(se_log_rr**2 + null_sd**2)\n",
    "    calibrated_results['HR_Calibrated'] = np.exp(log_rr - null_mean)\n",
    "    calibrated_results['HR_Cal_Low'] = np.exp((log_rr - null_mean) - 1.96 * calibrated_se)\n",
    "    calibrated_results['HR_Cal_High'] = np.exp((log_rr - null_mean) + 1.96 * calibrated_se)\n",
    "    \n",
    "    return calibrated_results\n",
    "\n",
    "# Function to plot the calibration (Funnel Plot)\n",
    "def plot_calibration(results_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot Negative Controls\n",
    "    ncs = results_df[results_df['Outcome'].str.startswith('NC_')]\n",
    "    log_rr_nc = np.log(ncs['HR_Cox'].astype(float))\n",
    "    se_nc = (np.log(ncs['HR_CI_High'].astype(float)) - np.log(ncs['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    plt.scatter(log_rr_nc, 1/se_nc, alpha=0.5, color='gray', label='Negative Controls')\n",
    "    \n",
    "    # Plot Main Outcomes\n",
    "    main = results_df[~results_df['Outcome'].str.startswith('NC_')]\n",
    "    log_rr_main = np.log(main['HR_Cox'].astype(float))\n",
    "    se_main = (np.log(main['HR_CI_High'].astype(float)) - np.log(main['HR_CI_Low'].astype(float))) / 3.92\n",
    "    \n",
    "    plt.scatter(log_rr_main, 1/se_main, color='red', s=100, label='Main Outcomes', zorder=10)\n",
    "    \n",
    "    # Plot Null Line (x=0 is HR=1)\n",
    "    plt.axvline(0, color='black', linestyle='--')\n",
    "    \n",
    "    # Plot Fitted Null Area (Mean +/- SD)\n",
    "    # We just draw the region around 0 to show visual calibration\n",
    "    # (If using the fitted values, draw vertical lines at null_mean)\n",
    "    \n",
    "    plt.xlabel(\"Log Hazard Ratio\")\n",
    "    plt.ylabel(\"Precision (1/SE)\")\n",
    "    plt.title(\"Empirical Calibration Funnel Plot\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b97670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Clinical Specifics (Labs & Outcomes) [Chunked Version]\n",
    "# =============================================================================\n",
    "print(\"\\n--- CELL 4: Clinical Specifics (Labs & Outcomes) ---\")\n",
    "\n",
    "# --- Helper Function for Chunking ---\n",
    "def get_data_in_chunks(sql_template, all_ids, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Splits the list of person_ids into smaller chunks to avoid BigQuery\n",
    "    query length limits (1MB).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    ids_list = sorted(list(set(all_ids))) # Ensure unique and list format\n",
    "    \n",
    "    print(f\"  Fetching data for {len(ids_list)} patients in chunks of {chunk_size}...\")\n",
    "    \n",
    "    # Loop through chunks\n",
    "    for i in range(0, len(ids_list), chunk_size):\n",
    "        chunk = ids_list[i : i + chunk_size]\n",
    "        chunk_str = \"(\" + \",\".join(map(str, chunk)) + \")\"\n",
    "        \n",
    "        # Inject the chunk of IDs into the placeholder\n",
    "        query = sql_template.replace(\"PLACEHOLDER_IDS\", chunk_str)\n",
    "        \n",
    "        try:\n",
    "            df_chunk = read_gbq(query, dialect=\"standard\")\n",
    "            results.append(df_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in chunk {i}: {e}\")\n",
    "            \n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# List of all patients in cohort\n",
    "all_cohort_ids = df_cohort.index.tolist()\n",
    "\n",
    "# --- A. Measurements: Strict eGFR vs Creatinine Separation ---\n",
    "\n",
    "# 1. Fetch eGFR\n",
    "# Note: We use PLACEHOLDER_IDS instead of injecting the huge list immediately\n",
    "sql_egfr_template = f\"\"\"\n",
    "SELECT person_id, measurement_date as date, value_as_number\n",
    "FROM `{WORKSPACE_CDR}.measurement`\n",
    "WHERE measurement_concept_id IN ({','.join(map(str, EGFR_CONCEPTS))})\n",
    "AND person_id IN PLACEHOLDER_IDS\n",
    "AND value_as_number > 0 AND value_as_number < 200\n",
    "\"\"\"\n",
    "print(\"Fetching eGFR...\")\n",
    "df_egfr_raw = get_data_in_chunks(sql_egfr_template, all_cohort_ids)\n",
    "df_egfr_raw['date'] = to_naive_utc_day(df_egfr_raw['date'])\n",
    "\n",
    "# 2. Fetch Creatinine\n",
    "sql_creat_template = f\"\"\"\n",
    "SELECT person_id, measurement_date as date, value_as_number\n",
    "FROM `{WORKSPACE_CDR}.measurement`\n",
    "WHERE measurement_concept_id IN ({','.join(map(str, CREATININE_CONCEPTS))})\n",
    "AND person_id IN PLACEHOLDER_IDS\n",
    "AND value_as_number > 0.1 AND value_as_number < 20\n",
    "\"\"\"\n",
    "print(\"Fetching Creatinine...\")\n",
    "df_creat_raw = get_data_in_chunks(sql_creat_template, all_cohort_ids)\n",
    "df_creat_raw['date'] = to_naive_utc_day(df_creat_raw['date'])\n",
    "\n",
    "# Merge to find baseline (Closest prior to index)\n",
    "df_dates = df_cohort[['index_date']].reset_index()\n",
    "\n",
    "# Function to get last value before index\n",
    "def get_baseline_lab(df_lab, df_index, col_name):\n",
    "    if df_lab.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    merged = df_lab.merge(df_index, on='person_id')\n",
    "    # Strictly prior to index\n",
    "    merged = merged[merged['date'] < merged['index_date']].sort_values('date')\n",
    "    return merged.groupby('person_id')['value_as_number'].last().rename(col_name)\n",
    "\n",
    "df_cohort['baseline_egfr'] = get_baseline_lab(df_egfr_raw, df_dates, 'baseline_egfr')\n",
    "df_cohort['baseline_creat'] = get_baseline_lab(df_creat_raw, df_dates, 'baseline_creat')\n",
    "\n",
    "# Categorical Definitions\n",
    "conditions = [\n",
    "    (df_cohort['baseline_egfr'] < 30),\n",
    "    (df_cohort['baseline_egfr'] >= 30) & (df_cohort['baseline_egfr'] < 45),\n",
    "    (df_cohort['baseline_egfr'] >= 45) & (df_cohort['baseline_egfr'] < 60),\n",
    "    (df_cohort['baseline_egfr'] >= 60)\n",
    "]\n",
    "df_cohort['egfr_cat'] = np.select(conditions, [0, 1, 2, 3], default=4) # 4 is Missing\n",
    "\n",
    "# --- B. Outcomes & Pre-Existing Conditions ---\n",
    "all_outcome_concepts = OUTCOME_CONCEPTS.union(THYROTOXICOSIS_IDS).union(ALL_NEGATIVE_CONTROL_IDS)\n",
    "\n",
    "# Note: We construct the UNION ALL inside the template, but both parts need PLACEHOLDER_IDS\n",
    "sql_outcomes_template = f\"\"\"\n",
    "SELECT person_id, condition_start_date as event_date, condition_concept_id\n",
    "FROM `{WORKSPACE_CDR}.condition_occurrence`\n",
    "WHERE condition_concept_id IN ({','.join(map(str, all_outcome_concepts))})\n",
    "AND person_id IN PLACEHOLDER_IDS\n",
    "UNION ALL\n",
    "SELECT person_id, death_date as event_date, 0 as condition_concept_id\n",
    "FROM `{WORKSPACE_CDR}.death`\n",
    "WHERE person_id IN PLACEHOLDER_IDS\n",
    "\"\"\"\n",
    "print(\"Fetching Outcomes & Events...\")\n",
    "df_events = get_data_in_chunks(sql_outcomes_template, all_cohort_ids)\n",
    "df_events['event_date'] = to_naive_utc_day(df_events['event_date'])\n",
    "df_events = df_events.merge(df_dates, on='person_id')\n",
    "\n",
    "# 1. Pre-existing Thyrotoxicosis\n",
    "pre_thyro = df_events[\n",
    "    (df_events['condition_concept_id'].isin(THYROTOXICOSIS_IDS)) & \n",
    "    (df_events['event_date'] < df_events['index_date'])\n",
    "]\n",
    "df_cohort['hx_thyrotoxicosis'] = 0\n",
    "df_cohort.loc[df_cohort.index.isin(pre_thyro['person_id']), 'hx_thyrotoxicosis'] = 1\n",
    "\n",
    "# 2. Standardized Outcome Dates\n",
    "# Death\n",
    "df_death = df_events[df_events['condition_concept_id'] == 0]\n",
    "df_cohort['date_DEATH'] = df_death.groupby('person_id')['event_date'].min()\n",
    "\n",
    "# Map Specific Outcomes\n",
    "for outcome, (concepts, window) in ANALYSIS_OUTCOMES.items():\n",
    "    if outcome in ['MORTALITY_30', 'MAE_30']: continue \n",
    "    \n",
    "    events = df_events[\n",
    "        (df_events['condition_concept_id'].isin(concepts)) & \n",
    "        (df_events['event_date'] >= df_events['index_date'])\n",
    "    ]\n",
    "    df_cohort[f'date_{outcome}'] = events.groupby('person_id')['event_date'].min()\n",
    "\n",
    "# Composite MAE_30\n",
    "mae_cols = ['date_AKI_30', 'date_NEW_DIALYSIS_90', 'date_DEATH']\n",
    "mae_valid = [c for c in mae_cols if c in df_cohort.columns]\n",
    "if mae_valid:\n",
    "    df_cohort['date_MAE_30'] = df_cohort[mae_valid].min(axis=1)\n",
    "\n",
    "# Negative Controls\n",
    "for name, concepts in NEGATIVE_CONTROLS.items():\n",
    "    events = df_events[\n",
    "        (df_events['condition_concept_id'].isin(concepts)) & \n",
    "        (df_events['event_date'] >= df_events['index_date'])\n",
    "    ]\n",
    "    df_cohort[f'date_{name}'] = events.groupby('person_id')['event_date'].min()\n",
    "\n",
    "print(\"Clinical Specifics Attached (Chunked).\")\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: High-Dimensional Feature Extraction (Robust High-RAM Version)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from pandas_gbq import read_gbq\n",
    "import gc\n",
    "\n",
    "print(\"\\n--- CELL 5: High-Dimensional Feature Extraction ---\")\n",
    "\n",
    "# 1. Setup & Helper Functions\n",
    "# ---------------------------------------------------------\n",
    "# Define Exclusions (Exposure itself)\n",
    "exclusions = ALL_IMAGING \n",
    "exclude_str = f\"({','.join(map(str, exclusions))})\"\n",
    "\n",
    "# Get list of ALL patient IDs from the cohort dataframe\n",
    "# This fixes the NameError\n",
    "all_cohort_ids = df_cohort.index.tolist()\n",
    "\n",
    "def get_data_in_chunks(sql_template, all_ids, chunk_size=4000):\n",
    "    \"\"\"Downloads data in chunks to satisfy BigQuery 1MB query limit.\"\"\"\n",
    "    results = []\n",
    "    ids_list = sorted(list(set(all_ids)))\n",
    "    print(f\"  Downloading data for {len(ids_list)} patients (Chunks of {chunk_size})...\")\n",
    "    \n",
    "    for i in range(0, len(ids_list), chunk_size):\n",
    "        chunk = ids_list[i : i + chunk_size]\n",
    "        # Robust string formatting for the IN clause\n",
    "        chunk_str = \"(\" + \",\".join(map(str, chunk)) + \")\"\n",
    "        query = sql_template.replace(\"PLACEHOLDER_IDS\", chunk_str)\n",
    "        try:\n",
    "            df_chunk = read_gbq(query, dialect=\"standard\")\n",
    "            results.append(df_chunk)\n",
    "            print(f\"    Chunk {i//chunk_size + 1} downloaded ({len(df_chunk)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in chunk {i}: {e}\")\n",
    "            \n",
    "    if not results: return pd.DataFrame()\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 2. SQL Construction (Template)\n",
    "# ---------------------------------------------------------\n",
    "# We use PLACEHOLDER_IDS instead of injecting the huge list\n",
    "sql_features_template = f\"\"\"\n",
    "WITH Cohort AS (\n",
    "    SELECT \n",
    "        p.person_id, \n",
    "        CAST(p.procedure_datetime AS DATE) as index_date\n",
    "    FROM `{WORKSPACE_CDR}.procedure_occurrence` p\n",
    "    WHERE p.person_id IN PLACEHOLDER_IDS\n",
    "    AND p.procedure_concept_id IN ({','.join(map(str, ALL_IMAGING))})\n",
    ")\n",
    "SELECT \n",
    "    c.person_id, \n",
    "    CAST(c.condition_concept_id AS STRING) as feature_id, \n",
    "    'COND' as domain\n",
    "FROM `{WORKSPACE_CDR}.condition_occurrence` c\n",
    "JOIN Cohort i ON c.person_id = i.person_id\n",
    "WHERE c.condition_start_date < i.index_date \n",
    "  AND c.condition_concept_id NOT IN {exclude_str}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    d.person_id, \n",
    "    CAST(d.drug_concept_id AS STRING) as feature_id, \n",
    "    'DRUG' as domain\n",
    "FROM `{WORKSPACE_CDR}.drug_exposure` d\n",
    "JOIN Cohort i ON d.person_id = i.person_id\n",
    "WHERE d.drug_exposure_start_date < i.index_date\n",
    "  AND d.drug_concept_id NOT IN {exclude_str}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    p.person_id, \n",
    "    CAST(p.procedure_concept_id AS STRING) as feature_id, \n",
    "    'PROC' as domain\n",
    "FROM `{WORKSPACE_CDR}.procedure_occurrence` p\n",
    "JOIN Cohort i ON p.person_id = i.person_id\n",
    "WHERE p.procedure_date < i.index_date\n",
    "  AND p.procedure_concept_id NOT IN {exclude_str}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    m.person_id, \n",
    "    CAST(m.measurement_concept_id AS STRING) as feature_id, \n",
    "    'MEAS' as domain\n",
    "FROM `{WORKSPACE_CDR}.measurement` m\n",
    "JOIN Cohort i ON m.person_id = i.person_id\n",
    "WHERE m.measurement_date < i.index_date\n",
    "  AND m.measurement_concept_id NOT IN {exclude_str}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    o.person_id, \n",
    "    CONCAT(CAST(o.observation_concept_id AS STRING), '_', CAST(COALESCE(o.value_as_concept_id, 0) AS STRING)) as feature_id, \n",
    "    'OBS' as domain\n",
    "FROM `{WORKSPACE_CDR}.observation` o\n",
    "JOIN Cohort i ON o.person_id = i.person_id\n",
    "WHERE o.observation_date < i.index_date\n",
    "  AND o.observation_concept_id NOT IN {exclude_str}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    dv.person_id, \n",
    "    CAST(dv.device_concept_id AS STRING) as feature_id, \n",
    "    'DEV' as domain\n",
    "FROM `{WORKSPACE_CDR}.device_exposure` dv\n",
    "JOIN Cohort i ON dv.person_id = i.person_id\n",
    "WHERE dv.device_exposure_start_date < i.index_date\n",
    "  AND dv.device_concept_id NOT IN {exclude_str}\n",
    "\"\"\"\n",
    "\n",
    "# 3. Execution (Chunked Download + High-Speed Processing)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Starting High-RAM Feature Extraction...\")\n",
    "\n",
    "# A. Download\n",
    "df_features = get_data_in_chunks(sql_features_template, all_cohort_ids)\n",
    "\n",
    "# B. Process (Vectorized)\n",
    "if not df_features.empty:\n",
    "    print(f\"Total raw features rows: {len(df_features):,}\")\n",
    "    \n",
    "    # Create PID mapping if missing\n",
    "    if 'pid_to_idx' not in locals():\n",
    "        pid_to_idx = {pid: i for i, pid in enumerate(df_cohort.index)}\n",
    "    \n",
    "    # Filter to cohort (safety)\n",
    "    df_features = df_features[df_features['person_id'].isin(pid_to_idx)].copy()\n",
    "    \n",
    "    # Vectorized string concat (Fast)\n",
    "    df_features['feature_name'] = df_features['domain'] + '_' + df_features['feature_id']\n",
    "    \n",
    "    # Count\n",
    "    feature_counts = df_features['feature_name'].value_counts()\n",
    "    \n",
    "    # Filter Prevalence >= 50\n",
    "    valid_feats_set = set(feature_counts[feature_counts >= 50].index)\n",
    "    feat_to_idx = {feat: i for i, feat in enumerate(sorted(list(valid_feats_set)))}\n",
    "    \n",
    "    print(f\"Unique Features: {len(feature_counts)}. Retained (>=50): {len(valid_feats_set)}\")\n",
    "    \n",
    "    # Final Filter\n",
    "    df_features_valid = df_features[df_features['feature_name'].isin(valid_feats_set)]\n",
    "    \n",
    "    # C. Build Matrix\n",
    "    print(\"Building Sparse Matrix...\")\n",
    "    row_indices = df_features_valid['person_id'].map(pid_to_idx).values\n",
    "    col_indices = df_features_valid['feature_name'].map(feat_to_idx).values\n",
    "    values = np.ones(len(row_indices))\n",
    "    \n",
    "    X_sparse = sp.coo_matrix(\n",
    "        (values, (row_indices, col_indices)),\n",
    "        shape=(len(df_cohort), len(valid_feats_set))\n",
    "    ).tocsr()\n",
    "    \n",
    "    # Binarize\n",
    "    X_sparse.data = np.ones_like(X_sparse.data)\n",
    "    \n",
    "    print(f\"Final Sparse Matrix Shape: {X_sparse.shape}\")\n",
    "    \n",
    "    # Clean Memory\n",
    "    del df_features, df_features_valid, row_indices, col_indices\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"CRITICAL WARNING: No feature history found for this cohort.\")\n",
    "    X_sparse = sp.csr_matrix((len(df_cohort), 0))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0216476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd039c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b58950",
   "metadata": {},
   "source": [
    "MIDPOINT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f61b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b73f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REVISED CELL: RESTRICT COHORT TO TARGET CLINICAL PROCEDURES (NO ANGIO)\n",
    "# =============================================================================\n",
    "print(\"--- FILTERING COHORT TO TARGET CLINICAL PROCEDURES (Excluding Angio) ---\")\n",
    "\n",
    "# 1. Define Procedure Sets\n",
    "\n",
    "# A. Routine Chest (Thorax)\n",
    "CHEST_CODES = {4327032, 3013610, 37117305, 3047921}\n",
    "\n",
    "# B. Routine Abdomen / Pelvis (Combined + Standalone)\n",
    "ABD_PEL_CODES = {\n",
    "    # Combined\n",
    "    21492176, 3047782, 4335400, 37109313, 3049940,\n",
    "    # Abdomen Only\n",
    "    4139745, 36713200, 3018999,\n",
    "    # Pelvis Only\n",
    "    4252907, 3019625, 40771605, 36713202, 3035568\n",
    "}\n",
    "\n",
    "# C. Neuro / Head (MRI/CT Brain)\n",
    "NEURO_CODES = {4197203, 36717294, 36713262, 36713243, 3024397}\n",
    "\n",
    "# D. Angiography (Defined but EXCLUDED)\n",
    "ANGIO_CODES = {3053128, 36713226, 45765683, 3053040}\n",
    "\n",
    "# 2. Combine for Master Filter\n",
    "# NOTE: We intentionally exclude ANGIO_CODES here due to low sample size\n",
    "TARGET_PROCEDURES = CHEST_CODES.union(ABD_PEL_CODES).union(NEURO_CODES)\n",
    "\n",
    "# 3. Apply Filter\n",
    "mask_keep = df_cohort['procedure_concept_id'].isin(TARGET_PROCEDURES)\n",
    "\n",
    "old_n = len(df_cohort)\n",
    "df_cohort = df_cohort[mask_keep].copy()\n",
    "new_n = len(df_cohort)\n",
    "\n",
    "# 4. Slice Sparse Matrix\n",
    "X_sparse = X_sparse[mask_keep.values, :]\n",
    "\n",
    "# 5. Clean Mappings\n",
    "pid_to_idx = {pid: i for i, pid in enumerate(df_cohort.index)}\n",
    "\n",
    "# 6. Assign Broad Group Labels\n",
    "conditions = [\n",
    "    df_cohort['procedure_concept_id'].isin(CHEST_CODES),\n",
    "    df_cohort['procedure_concept_id'].isin(ABD_PEL_CODES),\n",
    "    df_cohort['procedure_concept_id'].isin(NEURO_CODES)\n",
    "]\n",
    "choices = ['Routine Chest', 'Routine Abd/Pel', 'Neuro/Head']\n",
    "df_cohort['proc_group'] = np.select(conditions, choices, default='Other')\n",
    "\n",
    "print(f\"Original N: {old_n}\")\n",
    "print(f\"Filtered N: {new_n} (Removed {old_n - new_n} rows)\")\n",
    "print(f\"Counts by Group:\\n{df_cohort['proc_group'].value_counts()}\")\n",
    "print(f\"X_sparse new shape: {X_sparse.shape}\")\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Feature Engineering & Alignment (Optimized + TQDM)\n",
    "# =============================================================================\n",
    "print(\"\\n--- CELL 6: Feature Engineering & Alignment ---\")\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Setup Progress Bar\n",
    "pbar = tqdm(total=6, desc=\"Initializing\")\n",
    "\n",
    "# 1. Align Dataframe to Sparse Matrix\n",
    "pbar.set_description(\"Aligning Dataframes\")\n",
    "if 'pid_to_idx' not in locals():\n",
    "    raise ValueError(\"Missing pid_to_idx! Please run Cell 5.5.\")\n",
    "\n",
    "sorted_pids = sorted(pid_to_idx, key=pid_to_idx.get)\n",
    "df_cohort_aligned = df_cohort.loc[sorted_pids].copy()\n",
    "\n",
    "# Safety Check\n",
    "assert len(df_cohort_aligned) == X_sparse.shape[0], \"Row count mismatch between Dense and Sparse!\"\n",
    "assert df_cohort_aligned.index[0] == sorted_pids[0], \"Index alignment error!\"\n",
    "\n",
    "print(f\"Aligned Cohort N={len(df_cohort_aligned)}\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 2. Feature Engineering: Dense Covariates\n",
    "\n",
    "# A. Site Rate Smoothing\n",
    "pbar.set_description(\"Smoothing Site Rates\")\n",
    "if 'zip_code' in df_cohort_aligned.columns:\n",
    "    # Use float32 to save RAM if dataset is massive, otherwise default float is fine\n",
    "    site_counts = df_cohort_aligned.groupby('zip_code')['contrast_received'].agg(['mean', 'count'])\n",
    "    global_mean = df_cohort_aligned['contrast_received'].mean()\n",
    "    C_smooth = 10 \n",
    "    site_counts['smoothed_rate'] = (site_counts['mean'] * site_counts['count'] + global_mean * C_smooth) / (site_counts['count'] + C_smooth)\n",
    "    df_cohort_aligned['site_contrast_rate'] = df_cohort_aligned['zip_code'].map(site_counts['smoothed_rate']).fillna(global_mean)\n",
    "else:\n",
    "    df_cohort_aligned['site_contrast_rate'] = df_cohort_aligned['contrast_received'].mean()\n",
    "pbar.update(1)\n",
    "\n",
    "# B. Imputation & Scaling\n",
    "pbar.set_description(\"Imputing & Scaling\")\n",
    "# Handle Imputation\n",
    "for col in ['baseline_egfr', 'baseline_creat']:\n",
    "    if col in df_cohort_aligned.columns:\n",
    "        df_cohort_aligned[f'{col}_missing'] = df_cohort_aligned[col].isna().astype(float) # Direct to float\n",
    "        df_cohort_aligned[f'{col}_imputed'] = df_cohort_aligned[col].fillna(df_cohort_aligned[col].median())\n",
    "    else:\n",
    "        df_cohort_aligned[f'{col}_missing'] = 1.0\n",
    "        df_cohort_aligned[f'{col}_imputed'] = 0.0\n",
    "\n",
    "# Handle Scaling\n",
    "dense_cols_to_scale = ['age', 'site_contrast_rate', 'baseline_egfr_imputed', 'baseline_creat_imputed']\n",
    "dense_cols_to_scale = [c for c in dense_cols_to_scale if c in df_cohort_aligned.columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Output directly as float\n",
    "X_dense_scaled = scaler.fit_transform(df_cohort_aligned[dense_cols_to_scale].fillna(0))\n",
    "pbar.update(1)\n",
    "\n",
    "# C. Categorical Dummies \n",
    "# OPTIMIZATION: dtype=float here prevents a massive copy/cast later\n",
    "pbar.set_description(\"Generating Dummies\")\n",
    "\n",
    "if 'egfr_cat' in df_cohort_aligned.columns:\n",
    "    egfr_dummies = pd.get_dummies(df_cohort_aligned['egfr_cat'], prefix='egfr_cat', dtype=float)\n",
    "else:\n",
    "    egfr_dummies = pd.DataFrame()\n",
    "\n",
    "df_cohort_aligned['age_decile'] = pd.qcut(df_cohort_aligned['age'], q=10, labels=False, duplicates='drop')\n",
    "age_dummies = pd.get_dummies(df_cohort_aligned['age_decile'], prefix='age_decile', dtype=float)\n",
    "\n",
    "gender_dummies = pd.get_dummies(df_cohort_aligned['gender_concept_id'], prefix='gender', dtype=float)\n",
    "\n",
    "# Direct float conversion\n",
    "thyro_dummy = df_cohort_aligned[['hx_thyrotoxicosis']].astype(float)\n",
    "missing_flags = df_cohort_aligned[[c for c in df_cohort_aligned.columns if c.endswith('_missing')]].astype(float)\n",
    "pbar.update(1)\n",
    "\n",
    "# 3. Combine All Dense Features\n",
    "pbar.set_description(\"Concatenating Dense Matrix\")\n",
    "X_dense_list = [\n",
    "    pd.DataFrame(X_dense_scaled, index=df_cohort_aligned.index, columns=dense_cols_to_scale),\n",
    "    egfr_dummies,\n",
    "    age_dummies,\n",
    "    gender_dummies,\n",
    "    thyro_dummy,\n",
    "    missing_flags\n",
    "]\n",
    "\n",
    "# OPTIMIZATION: No .astype(float) needed here anymore, saving one full memory write\n",
    "df_dense_final = pd.concat(X_dense_list, axis=1)\n",
    "X_dense = sp.csr_matrix(df_dense_final.values)\n",
    "pbar.update(1)\n",
    "\n",
    "# 4. Final Stack (Dense + Sparse)\n",
    "pbar.set_description(\"Final Stack (HStack)\")\n",
    "X_all = sp.hstack([X_dense, X_sparse], format='csr')\n",
    "pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "clean_mem()\n",
    "print(f\"Final Input Matrix Shape: {X_all.shape}\")\n",
    "print(f\"Dense Features included ({df_dense_final.shape[1]}): {list(df_dense_final.columns[:10])} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773109c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 7A: Hyperparameter Tuning on 10k Subsample ---\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "T = df_cohort_aligned['contrast_received'].values\n",
    "N = len(T)\n",
    "\n",
    "print(f\"Full cohort: N={N}, P={X_all.shape[1]}\")\n",
    "\n",
    "# --- 1. Choose a ~10k stratified subsample ---\n",
    "target_n = 10_000\n",
    "if N <= target_n:\n",
    "    print(\"N <= 10k, using full cohort for tuning.\")\n",
    "    idx_sub = np.arange(N)\n",
    "else:\n",
    "    frac = target_n / N\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=frac,\n",
    "        random_state=42\n",
    "    )\n",
    "    _, idx_sub = next(sss.split(X_all, T))\n",
    "    print(f\"Using subsample of size {len(idx_sub)} for tuning.\")\n",
    "\n",
    "X_sub = X_all[idx_sub]\n",
    "T_sub = T[idx_sub]\n",
    "\n",
    "# --- 2. Define base logistic model ---\n",
    "# saga supports L1/L2 with large, possibly sparse, high-dim data.\n",
    "base_logit = LogisticRegression(\n",
    "    penalty='l1',              # we'll start with L1; could swap to 'l2'\n",
    "    solver='saga',             # good for large P, supports l1\n",
    "    max_iter=3000,\n",
    "    class_weight='balanced',   # very standard for PS in imbalanced settings\n",
    "    n_jobs=-1,                 # use all visible cores inside each fit\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Grid over C (log-scale) ---\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 0, 6)   # 0.001, 0.0032, 0.01, 0.032, 0.1, 1.0\n",
    "}\n",
    "\n",
    "print(\"Tuning C on subsample using 3-fold CV (scoring=roc_auc)...\")\n",
    "gs = GridSearchCV(\n",
    "    estimator=base_logit,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gs.fit(X_sub, T_sub)\n",
    "\n",
    "best_C = gs.best_params_[\"C\"]\n",
    "best_score = gs.best_score_\n",
    "\n",
    "print(f\"Best C: {best_C:.4g} (mean CV AUC: {best_score:.4f})\")\n",
    "\n",
    "# We'll reuse this C downstream\n",
    "C_best = float(best_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 7B: Cross-Fitted PS (Parallel Over Folds, Tunable Core Use) ---\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "T = df_cohort_aligned['contrast_received'].values\n",
    "N, P = X_all.shape\n",
    "print(f\"Training cross-fitted PS on full cohort (N={N}, P={P}, C={C_best})\")\n",
    "\n",
    "outer_k = 5\n",
    "cv_outer = StratifiedKFold(\n",
    "    n_splits=outer_k,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ---- CORE / THREAD SETTINGS ----\n",
    "threads_per_fold = 6  \n",
    "n_folds_workers = min(outer_k, 32 // threads_per_fold or 1)\n",
    "\n",
    "print(f\"Using up to {n_folds_workers} folds in parallel, \"\n",
    "      f\"{threads_per_fold} threads per fold (target ~{n_folds_workers * threads_per_fold} cores)\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(threads_per_fold)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(threads_per_fold)\n",
    "\n",
    "def fit_one_fold(fold_id, train_idx, test_idx, X, T, C_best):\n",
    "    \"\"\"Train L1-logistic on one fold and return PS for its test indices.\"\"\"\n",
    "    t0 = time.time()\n",
    "    print(f\"[Fold {fold_id}] start: train={len(train_idx)}, test={len(test_idx)}\")\n",
    "\n",
    "    logit = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='saga',\n",
    "        C=C_best,\n",
    "        max_iter=1500,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=threads_per_fold,   # <-- threads per fold\n",
    "        random_state=42 + fold_id\n",
    "    )\n",
    "    logit.fit(X[train_idx], T[train_idx])\n",
    "    ps_fold = logit.predict_proba(X[test_idx])[:, 1]\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"[Fold {fold_id}] done in {(t1 - t0)/60:.2f} min\")\n",
    "\n",
    "    return fold_id, test_idx, ps_fold\n",
    "\n",
    "# Prepare fold tasks\n",
    "fold_tasks = [\n",
    "    (fold_id, train_idx, test_idx)\n",
    "    for fold_id, (train_idx, test_idx)\n",
    "    in enumerate(cv_outer.split(X_all, T), start=1)\n",
    "]\n",
    "\n",
    "# Run folds in parallel\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "print(f\"Running {outer_k} folds with joblib (n_jobs={n_folds_workers})...\")\n",
    "results = Parallel(\n",
    "    n_jobs=n_folds_workers,\n",
    "    verbose=10,\n",
    "    backend=\"loky\"\n",
    ")(\n",
    "    delayed(fit_one_fold)(fold_id, train_idx, test_idx, X_all, T, C_best)\n",
    "    for (fold_id, train_idx, test_idx) in fold_tasks\n",
    ")\n",
    "\n",
    "# Merge PS\n",
    "ps = np.zeros(N, dtype=float)\n",
    "for fold_id, test_idx, ps_fold in results:\n",
    "    ps[test_idx] = ps_fold\n",
    "\n",
    "df_cohort_aligned['ps'] = ps\n",
    "\n",
    "print(\"Fitting final global model on full data for interpretation...\")\n",
    "lsps_model = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    solver='saga',\n",
    "    C=C_best,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=999\n",
    ")\n",
    "lsps_model.fit(X_all, T)\n",
    "\n",
    "print(\"Cross-fitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"\\n=== CHECKPOINT: Saving Model & Cross-Fitted Scores ===\")\n",
    "\n",
    "# 1. Save the Global Model (for coefficients/interpretation)\n",
    "joblib.dump(lsps_model, 'lsps_model_global.joblib')\n",
    "print(\"Saved Global Logistic Model (lsps_model_global.joblib)\")\n",
    "\n",
    "# 2. Save the Dataframe WITH the 'ps' column\n",
    "# We need to save the aligned version because it matches the X matrix rows\n",
    "# and contains the critical 'ps' and 'iptw' columns.\n",
    "df_cohort_aligned.to_parquet('df_cohort_with_ps.parquet')\n",
    "print(\"Saved Cohort with PS scores (df_cohort_with_ps.parquet)\")\n",
    "\n",
    "# 3. Save the C_best parameter (just in case)\n",
    "joblib.dump(C_best, 'C_best_param.joblib')\n",
    "\n",
    "print(\"Checkpoint Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e22e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7_LOAD: Reload Model & Scores (Skip Training)\n",
    "# =============================================================================\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- RELOADING PS MODEL & SCORES ---\")\n",
    "\n",
    "# 1. Load the Dataframe with PS\n",
    "# This restores the Cross-Fitted scores (crucial for valid inference)\n",
    "df_cohort_aligned = pd.read_parquet('df_cohort_with_ps.parquet')\n",
    "print(f\"Restored dataframe with PS. Shape: {df_cohort_aligned.shape}\")\n",
    "\n",
    "# 2. Load the Global Model\n",
    "# This restores the coefficients for interpretation\n",
    "lsps_model = joblib.load('lsps_model_global.joblib')\n",
    "print(\"Restored Global Model.\")\n",
    "\n",
    "# 3. Load Hyperparams\n",
    "try:\n",
    "    C_best = joblib.load('C_best_param.joblib')\n",
    "    print(f\"Restored C_best: {C_best}\")\n",
    "except:\n",
    "    print(\"C_best not found, setting default.\")\n",
    "    C_best = 0.1 # Default fallback\n",
    "\n",
    "# 4. Consistency Check\n",
    "# Ensure the dataframe aligns with the X_all matrix you just generated in Cell 6\n",
    "# (This assumes you ran Cells 1-6 first)\n",
    "if 'X_all' in locals():\n",
    "    assert len(df_cohort_aligned) == X_all.shape[0], \"Row count mismatch! Did you re-run Cell 6?\"\n",
    "    \n",
    "    # Re-define T for downstream cells\n",
    "    T = df_cohort_aligned['contrast_received'].values\n",
    "    print(\"Consistency check passed. Ready for Diagnostics (Cell 7C).\")\n",
    "else:\n",
    "    print(\"WARNING: X_all variable not found in RAM. Please run Cell 6 (Feature Engineering) before proceeding to Love Plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 7C: Diagnostics, Trimming, Weights, ESS ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# PS diagnostics\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.kdeplot(\n",
    "    df_cohort_aligned.loc[df_cohort_aligned['contrast_received'] == 0, 'ps'],\n",
    "    fill=True, alpha=0.3, label='Control'\n",
    ")\n",
    "sns.kdeplot(\n",
    "    df_cohort_aligned.loc[df_cohort_aligned['contrast_received'] == 1, 'ps'],\n",
    "    fill=True, alpha=0.3, label='Treated'\n",
    ")\n",
    "plt.title(\"Propensity Score Overlap (Cross-Fitted, saga/l1)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Trimming\n",
    "ps = df_cohort_aligned['ps'].values\n",
    "mask_keep = (ps > 0.025) & (ps < 0.975)\n",
    "df_final = df_cohort_aligned[mask_keep].copy()\n",
    "X_final = X_all[mask_keep]\n",
    "T_final = T[mask_keep]\n",
    "ps_final = ps[mask_keep]\n",
    "\n",
    "print(f\"Original N: {N}\")\n",
    "print(f\"Trimmed  N: {len(df_final)} (Removed {N - len(df_final)})\")\n",
    "\n",
    "# IPW weights (stabilized)\n",
    "p_t = T_final.mean()\n",
    "weights = np.where(\n",
    "    T_final == 1,\n",
    "    p_t / ps_final,\n",
    "    (1 - p_t) / (1 - ps_final)\n",
    ")\n",
    "df_final['iptw'] = weights\n",
    "\n",
    "# Effective sample size\n",
    "ess = calculate_ess(weights)\n",
    "print(f\"Effective Sample Size (ESS): {ess:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIAGNOSTIC ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get Feature Names\n",
    "# Dense names from Cell 6\n",
    "dense_names = list(df_dense_final.columns)\n",
    "# Sparse names from the mapping in Cell 5.5\n",
    "# We need to invert the feat_to_idx dictionary\n",
    "idx_to_feat = {v: k for k, v in feat_to_idx.items()}\n",
    "sparse_names = [idx_to_feat[i] for i in range(len(feat_to_idx))]\n",
    "all_feature_names = dense_names + sparse_names\n",
    "\n",
    "# 2. Get Coefficients from the Lasso Model\n",
    "# The model is 'lsps_cv' from Cell 7\n",
    "coefs = lsps_model.coef_[0]\n",
    "\n",
    "# 3. Sort and Display\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Coefficient': coefs,\n",
    "    'Abs_Coef': np.abs(coefs)\n",
    "})\n",
    "\n",
    "print(\"\\n--- TOP 30 PREDICTORS OF RECEIVING CONTRAST ---\")\n",
    "print(coef_df.sort_values('Coefficient', ascending=False).head(30)[['Feature', 'Coefficient']])\n",
    "\n",
    "print(\"\\n--- TOP 30 PREDICTORS OF WITHHOLDING CONTRAST ---\")\n",
    "print(coef_df.sort_values('Coefficient', ascending=True).head(30)[['Feature', 'Coefficient']])\n",
    "\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # CELL 7.5: Feature Timing & Leakage Analysis (Days Before Index)\n",
    "# # =============================================================================\n",
    "# print(\"\\n--- CELL 7.5: Feature Timing Analysis (Leakage Check) ---\")\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from pandas_gbq import read_gbq\n",
    "\n",
    "# # --- 1. Define Helper Function ---\n",
    "# def get_data_in_chunks(sql_template, all_ids, chunk_size=5000):\n",
    "#     \"\"\"\n",
    "#     Downloads data in chunks to satisfy BigQuery query length limits.\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     ids_list = sorted(list(set(all_ids)))\n",
    "    \n",
    "#     for i in range(0, len(ids_list), chunk_size):\n",
    "#         chunk = ids_list[i : i + chunk_size]\n",
    "#         chunk_str = \"(\" + \",\".join(map(str, chunk)) + \")\"\n",
    "#         query = sql_template.replace(\"PLACEHOLDER_IDS\", chunk_str)\n",
    "        \n",
    "#         try:\n",
    "#             df_chunk = read_gbq(query, dialect=\"standard\")\n",
    "#             results.append(df_chunk)\n",
    "#         except Exception as e:\n",
    "#             print(f\"    Error in chunk {i}: {e}\")\n",
    "            \n",
    "#     if not results:\n",
    "#         return pd.DataFrame()\n",
    "        \n",
    "#     return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# # --- 2. Select Features to Check (Top 25 Pos + Top 25 Neg) ---\n",
    "# if 'coef_df' not in locals() or 'df_cohort_aligned' not in locals():\n",
    "#     raise ValueError(\"Critical variables missing. Run previous Diagnostic cells first.\")\n",
    "\n",
    "# # Split into Positive and Negative sets immediately\n",
    "# top_pos = coef_df.sort_values('Coefficient', ascending=False).head(25).copy()\n",
    "# top_neg = coef_df.sort_values('Coefficient', ascending=True).head(25).copy()\n",
    "\n",
    "# # Combine for querying (efficiency), but keep track of sets\n",
    "# top_features = pd.concat([top_pos, top_neg])\n",
    "# top_features = top_features[top_features['Feature'].str.contains('_')].copy() # Filter out dense vars\n",
    "# features_to_check = top_features['Feature'].tolist()\n",
    "\n",
    "# print(f\"Analyzing timing for {len(features_to_check)} top features...\")\n",
    "\n",
    "# # --- 3. Map Feature Names to SQL Tables ---\n",
    "# domain_map = {\n",
    "#     'PROC': ('procedure_occurrence', 'procedure_concept_id', 'procedure_date'),\n",
    "#     'COND': ('condition_occurrence', 'condition_concept_id', 'condition_start_date'),\n",
    "#     'DRUG': ('drug_exposure', 'drug_concept_id', 'drug_exposure_start_date'),\n",
    "#     'MEAS': ('measurement', 'measurement_concept_id', 'measurement_date'),\n",
    "#     'OBS':  ('observation', 'observation_concept_id', 'observation_date'),\n",
    "#     'DEV':  ('device_exposure', 'device_concept_id', 'device_exposure_start_date')\n",
    "# }\n",
    "\n",
    "# # Group for querying\n",
    "# feats_by_domain = {}\n",
    "# for f in features_to_check:\n",
    "#     parts = f.split('_')\n",
    "#     domain = parts[0]\n",
    "#     concept_id = parts[1]\n",
    "#     if domain in domain_map:\n",
    "#         if domain not in feats_by_domain: feats_by_domain[domain] = []\n",
    "#         feats_by_domain[domain].append(concept_id)\n",
    "\n",
    "# # --- 4. Query & Calculate Lags ---\n",
    "# timing_stats = []\n",
    "# cohort_ids = df_cohort_aligned.index.tolist()\n",
    "\n",
    "# for domain, concept_ids in feats_by_domain.items():\n",
    "#     if not concept_ids: continue\n",
    "    \n",
    "#     table, col_id, col_date = domain_map[domain]\n",
    "#     ids_str = \",\".join(list(set(concept_ids)))\n",
    "    \n",
    "#     sql_raw = f\"\"\"\n",
    "#     SELECT \n",
    "#         person_id,\n",
    "#         CAST({col_id} AS STRING) as concept_id,\n",
    "#         {col_date} as event_date\n",
    "#     FROM `{os.environ[\"WORKSPACE_CDR\"]}.{table}`\n",
    "#     WHERE {col_id} IN ({ids_str})\n",
    "#     AND person_id IN PLACEHOLDER_IDS\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # print(f\"  Fetching timing data for {domain}...\")\n",
    "#     df_raw = get_data_in_chunks(sql_raw, cohort_ids, chunk_size=5000)\n",
    "    \n",
    "#     if df_raw.empty: continue\n",
    "\n",
    "#     # Process Dates\n",
    "#     df_raw['event_date'] = pd.to_datetime(df_raw['event_date']).dt.tz_localize(None)\n",
    "#     idx_map = df_cohort_aligned['index_date'].reset_index()\n",
    "#     df_raw = df_raw.merge(idx_map, on='person_id', how='inner')\n",
    "    \n",
    "#     # Strict Filter: Must be BEFORE index\n",
    "#     df_raw = df_raw[df_raw['event_date'] < df_raw['index_date']]\n",
    "#     df_raw['days_before'] = (df_raw['index_date'] - df_raw['event_date']).dt.days\n",
    "    \n",
    "#     # Get most recent event per person/concept\n",
    "#     df_lag = df_raw.groupby(['person_id', 'concept_id'])['days_before'].min().reset_index()\n",
    "    \n",
    "#     # Calculate Percentiles\n",
    "#     for cid, group in df_lag.groupby('concept_id'):\n",
    "#         lags = group['days_before'].values\n",
    "#         matching_feats = [f for f in features_to_check if f.startswith(f\"{domain}_{cid}\")]\n",
    "        \n",
    "#         for feat_name in matching_feats:\n",
    "#             timing_stats.append({\n",
    "#                 'Feature': feat_name,\n",
    "#                 'N_Events': len(lags),\n",
    "#                 'P01': np.percentile(lags, 1),\n",
    "#                 'P05': np.percentile(lags, 5),\n",
    "#                 'P50_Median': np.percentile(lags, 50),\n",
    "#                 'P95': np.percentile(lags, 95)\n",
    "#             })\n",
    "\n",
    "# # --- 5. Merge, Split & Display ---\n",
    "# if timing_stats:\n",
    "#     df_timing = pd.DataFrame(timing_stats)\n",
    "    \n",
    "#     # Merge timing info back into the original split dataframes\n",
    "#     cols_show = ['Feature', 'Coefficient', 'N_Events', 'P01', 'P05', 'P50_Median', 'P95']\n",
    "    \n",
    "#     df_pos_final = top_pos.merge(df_timing, on='Feature', how='left')\n",
    "#     df_neg_final = top_neg.merge(df_timing, on='Feature', how='left')\n",
    "    \n",
    "#     print(\"\\n--- TOP POSITIVE PREDICTORS (Predicts RECEIVING Contrast) ---\")\n",
    "#     with pd.option_context('display.max_rows', None, 'display.float_format', '{:.1f}'.format):\n",
    "#         print(df_pos_final[cols_show].to_string(index=False))\n",
    "\n",
    "#     print(\"\\n--- TOP NEGATIVE PREDICTORS (Predicts WITHHOLDING Contrast) ---\")\n",
    "#     with pd.option_context('display.max_rows', None, 'display.float_format', '{:.1f}'.format):\n",
    "#         print(df_neg_final[cols_show].to_string(index=False))\n",
    "# else:\n",
    "#     print(\"No timing data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 7.5: POST-WEIGHTING EQUIPOISE & OVERLAP DIAGNOSTICS ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_overlap_coefficient(data0, data1, weights0, weights1, bins=100):\n",
    "    \"\"\"Calculates the overlapping area of two weighted density distributions (0 to 1).\"\"\"\n",
    "    # Create common bin edges\n",
    "    min_val = min(data0.min(), data1.min())\n",
    "    max_val = max(data0.max(), data1.max())\n",
    "    bins_edges = np.linspace(min_val, max_val, bins)\n",
    "    \n",
    "    # Calculate weighted histograms\n",
    "    hist0, _ = np.histogram(data0, bins=bins_edges, weights=weights0, density=True)\n",
    "    hist1, _ = np.histogram(data1, bins=bins_edges, weights=weights1, density=True)\n",
    "    \n",
    "    # Calculate intersection area (approximate integration)\n",
    "    bin_width = bins_edges[1] - bins_edges[0]\n",
    "    overlap_area = np.sum(np.minimum(hist0, hist1)) * bin_width\n",
    "    return overlap_area\n",
    "\n",
    "# Prepare Data\n",
    "ps_control = df_final[df_final['contrast_received']==0]['ps']\n",
    "w_control = df_final[df_final['contrast_received']==0]['iptw']\n",
    "ps_treated = df_final[df_final['contrast_received']==1]['ps']\n",
    "w_treated = df_final[df_final['contrast_received']==1]['iptw']\n",
    "\n",
    "# Calculate Metrics\n",
    "overlap_score = calculate_overlap_coefficient(ps_control, ps_treated, w_control, w_treated)\n",
    "\n",
    "print(f\"\\n--- EQUIPOISE DIAGNOSTICS ---\")\n",
    "print(f\"Distribution Overlap Coefficient: {overlap_score:.3f} (0=Separated, 1=Perfect Match)\")\n",
    "if overlap_score < 0.1:\n",
    "    print(\"WARNING: Poor overlap. Estimates relies heavily on extrapolation.\")\n",
    "elif overlap_score > 0.5:\n",
    "    print(\"SUCCESS: Strong clinical equipoise between groups.\")\n",
    "\n",
    "# --- PLOT ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot 1: Unweighted (Raw Propensity)\n",
    "# We use x=... explicitly here too for consistency\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(x=ps_control, fill=True, label='Withheld (Raw)', color='blue', alpha=0.3)\n",
    "sns.kdeplot(x=ps_treated, fill=True, label='Received (Raw)', color='red', alpha=0.3)\n",
    "plt.title(\"Before Weighting: Selection Bias\")\n",
    "plt.xlabel(\"Propensity Score\")\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "# Plot 2: Weighted (Pseudo-Population)\n",
    "# FIX: Added 'x=' before ps_control and ps_treated\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(x=ps_control, weights=w_control, fill=True, label='Withheld (Weighted)', color='blue', alpha=0.3)\n",
    "sns.kdeplot(x=ps_treated, weights=w_treated, fill=True, label='Received (Weighted)', color='red', alpha=0.3)\n",
    "plt.title(f\"After Weighting: Pseudo-Population\\nOverlap Coeff: {overlap_score:.2f}\")\n",
    "plt.xlabel(\"Propensity Score\")\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e5b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e27c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 8: LOVE PLOT ---\n",
    "\n",
    "def get_smd(X, t, w):\n",
    "    # Weighted Means\n",
    "    mu_1, var_1 = sparse_weighted_mean_var(X[t==1], w[t==1])\n",
    "    mu_0, var_0 = sparse_weighted_mean_var(X[t==0], w[t==0])\n",
    "    \n",
    "    # Pooled SD\n",
    "    pooled_sd = np.sqrt((var_1 + var_0) / 2)\n",
    "    pooled_sd[pooled_sd == 0] = 1e-6 # Avoid div/0\n",
    "    \n",
    "    return np.abs((mu_1 - mu_0) / pooled_sd)\n",
    "\n",
    "print(\"Calculating Balance...\")\n",
    "# Unweighted\n",
    "smd_unw = get_smd(X_final, T_final, np.ones(len(T_final)))\n",
    "# Weighted\n",
    "smd_w = get_smd(X_final, T_final, df_final['iptw'].values)\n",
    "\n",
    "# Plot top 50 imbalanced features\n",
    "top_idx = np.argsort(smd_unw)[-50:]\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.scatter(smd_unw[top_idx], range(50), label='Unadjusted', alpha=0.6)\n",
    "plt.scatter(smd_w[top_idx], range(50), label='Adjusted', alpha=0.8)\n",
    "plt.axvline(0.1, color='r', linestyle='--')\n",
    "plt.title(\"Covariate Balance (Top 50 Variates)\")\n",
    "plt.xlabel(\"Absolute SMD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Cross-Fitted SuperLearner AIPW Engine (Sklearn Stacking)\n",
    "# =============================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "print(\"\\n--- CELL 9: Super Learner AIPW Engine (Sklearn Stacking) ---\")\n",
    "\n",
    "def calculate_e_value(rr_or_hr):\n",
    "    \"\"\"Calculates E-Value for unmeasured confounding.\"\"\"\n",
    "    if rr_or_hr <= 1: return 1\n",
    "    return rr_or_hr + np.sqrt(rr_or_hr * (rr_or_hr - 1))\n",
    "\n",
    "def get_super_learner(n_jobs_inner=1):\n",
    "    \"\"\"\n",
    "    Returns an sklearn StackingClassifier designed for sparse data.\n",
    "    Combines:\n",
    "      1. Lasso (Linear, good for rare codes)\n",
    "      2. Random Forest (Non-linear, good for interactions)\n",
    "    \"\"\"\n",
    "    # 1. Base Learners\n",
    "    # We use 'liblinear' for Lasso as it handles sparse matrices efficiently.\n",
    "    # We limit RF depth slightly to prevent memory explosion with 70k features.\n",
    "    estimators = [\n",
    "        ('lasso', LogisticRegression(penalty='l1', solver='liblinear', C=0.2, \n",
    "                                     class_weight='balanced', max_iter=2000)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=20, \n",
    "                                      class_weight='balanced', n_jobs=n_jobs_inner))\n",
    "    ]\n",
    "    \n",
    "    # 2. The Stack (Meta-Learner)\n",
    "    # Uses internal CV to learn how to best combine Lasso and RF predictions\n",
    "    # passthrough=False means the meta-learner only sees the predictions of base learners\n",
    "    stack = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression(), \n",
    "        cv=3,  # Internal 3-fold CV to train the combiner\n",
    "        n_jobs=n_jobs_inner,\n",
    "        passthrough=False\n",
    "    )\n",
    "    return stack\n",
    "\n",
    "def _fit_super_learner_fold(train_idx, eval_idx, X_sparse, T_full, Y_full):\n",
    "    \"\"\"\n",
    "    Worker function for a single fold of Cross-Fitting.\n",
    "    Fits the Super Learner on training data, predicts on eval data.\n",
    "    \"\"\"\n",
    "    # 1. Slice Data (Sparse Slicing)\n",
    "    X_train, X_eval = X_sparse[train_idx], X_sparse[eval_idx]\n",
    "    T_train, T_eval = T_full[train_idx], T_full[eval_idx]\n",
    "    Y_train, Y_eval = Y_full[train_idx], Y_full[eval_idx]\n",
    "    \n",
    "    # Note: We use n_jobs_inner=4 for models to speed up RF training inside the fold.\n",
    "    # Total threads = n_folds (outer) * n_jobs_inner.\n",
    "    \n",
    "    # --- 2. Propensity Score Model (Pi) ---\n",
    "    sl_ps = get_super_learner(n_jobs_inner=4)\n",
    "    sl_ps.fit(X_train, T_train)\n",
    "    pi_hat = sl_ps.predict_proba(X_eval)[:, 1]\n",
    "    # Clip for stability (AIPW requirement)\n",
    "    pi_hat = np.clip(pi_hat, 0.025, 0.975)\n",
    "    \n",
    "    # --- 3. Outcome Models (Mu) ---\n",
    "    # We must separate T=0 and T=1 to learn the counterfactuals\n",
    "    mask0 = (T_train == 0)\n",
    "    mask1 = (T_train == 1)\n",
    "    \n",
    "    # Mu0 (Outcome if No Contrast)\n",
    "    sl_mu0 = get_super_learner(n_jobs_inner=4)\n",
    "    sl_mu0.fit(X_train[mask0], Y_train[mask0])\n",
    "    mu0_hat = sl_mu0.predict_proba(X_eval)[:, 1]\n",
    "    \n",
    "    # Mu1 (Outcome if Contrast)\n",
    "    sl_mu1 = get_super_learner(n_jobs_inner=4)\n",
    "    sl_mu1.fit(X_train[mask1], Y_train[mask1])\n",
    "    mu1_hat = sl_mu1.predict_proba(X_eval)[:, 1]\n",
    "    \n",
    "    # --- 4. Compute Efficient Influence Function (EIF) ---\n",
    "    # Formula: (mu1 - mu0) + T(Y-mu1)/pi - (1-T)(Y-mu0)/(1-pi)\n",
    "    term1 = mu1_hat - mu0_hat # Risk Difference\n",
    "    term2 = (T_eval * (Y_eval - mu1_hat)) / pi_hat\n",
    "    term3 = ((1 - T_eval) * (Y_eval - mu0_hat)) / (1 - pi_hat)\n",
    "    eif_chunk = term1 + term2 - term3\n",
    "    \n",
    "    return eval_idx, mu0_hat, mu1_hat, pi_hat, eif_chunk\n",
    "\n",
    "def run_cross_fitted_aipw(X_sparse_matrix, T_full, Y_full, n_folds=5):\n",
    "    \"\"\"\n",
    "    Main driver for K-Fold Cross-Fitting.\n",
    "    Returns: stats (dict), predictions (dict)\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    n = len(T_full)\n",
    "    \n",
    "    # Storage arrays\n",
    "    mu0_hat = np.zeros(n)\n",
    "    mu1_hat = np.zeros(n)\n",
    "    pi_hat  = np.zeros(n)\n",
    "    eif_val = np.zeros(n)\n",
    "    \n",
    "    print(f\"  Running {n_folds}-Fold Super Learner (StackingClassifier) in PARALLEL...\")\n",
    "    print(f\"  (This utilizes high compute: Lasso + Random Forest Stacking)\")\n",
    "\n",
    "    # --- Parallel Execution ---\n",
    "    # Runs the 5 folds simultaneously.\n",
    "    results = Parallel(n_jobs=n_folds, verbose=10)(\n",
    "        delayed(_fit_super_learner_fold)(train_idx, eval_idx, X_sparse_matrix, T_full, Y_full)\n",
    "        for train_idx, eval_idx in kf.split(X_sparse_matrix, T_full)\n",
    "    )\n",
    "    \n",
    "    # --- Aggregate Results ---\n",
    "    for eval_idx, mu0_c, mu1_c, pi_c, eif_c in results:\n",
    "        mu0_hat[eval_idx] = mu0_c\n",
    "        mu1_hat[eval_idx] = mu1_c\n",
    "        pi_hat[eval_idx]  = pi_c\n",
    "        eif_val[eval_idx] = eif_c\n",
    "\n",
    "    # --- Statistics & Inference ---\n",
    "    ate = np.mean(eif_val)\n",
    "    se = np.std(eif_val) / np.sqrt(n)\n",
    "    # Z-test P-value\n",
    "    p_value = 2 * (1 - norm.cdf(np.abs(ate / se))) if se > 0 else 0.0\n",
    "    \n",
    "    # Risk Estimates (Population Averages)\n",
    "    risk_1 = np.mean(mu1_hat)\n",
    "    risk_0 = np.mean(mu0_hat)\n",
    "    rr = risk_1 / risk_0 if risk_0 > 0 else 0.0\n",
    "    \n",
    "    # Effective Sample Size (Kish)\n",
    "    weights = np.where(T_full==1, 1/pi_hat, 1/(1-pi_hat))\n",
    "    ess = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n",
    "    \n",
    "    # E-Value Calculation\n",
    "    # Maps RR < 1 to equivalent risk increase for formula\n",
    "    e_calc_rr = 1/rr if (rr < 1 and rr > 0) else rr\n",
    "    e_val = calculate_e_value(e_calc_rr) if not np.isnan(rr) else 1.0\n",
    "\n",
    "    stats = {\n",
    "        'ATE': ate, 'SE': se, 'P_Value': p_value,\n",
    "        'Risk_1': risk_1, 'Risk_0': risk_0, \n",
    "        'RR': rr, 'ESS': ess, 'E_Value': e_val,\n",
    "        'CI_Lower': ate - 1.96*se, 'CI_Upper': ate + 1.96*se\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "        'mu0': mu0_hat, 'mu1': mu1_hat, 'pi': pi_hat, 'eif': eif_val\n",
    "    }\n",
    "    \n",
    "    return stats, predictions\n",
    "\n",
    "print(\"Super Learner Engine (Sklearn) Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Policies\n",
    "# Policies act on the 'df_final' cohort (covariates) to output a decision vector d âˆˆ {0, 1}^n\n",
    "# 0 = Withhold all contrast, 1 = Give contrast\n",
    "#\n",
    "# NOTE: ACR 2024 guidance is about *which* contrast (iodinated vs GBCA),\n",
    "# not \"withhold vs give\". The eGFR-based rules below are *toy* withholding\n",
    "# rules for counterfactual exploration, not literal guideline implementations.\n",
    "\n",
    "def policy_current(df):\n",
    "    \"\"\"Current Practice: The observed decision.\"\"\"\n",
    "    return df['contrast_received'].values\n",
    "\n",
    "def policy_always(df):\n",
    "    \"\"\"Extreme policy: Always give contrast (100% contrast use).\"\"\"\n",
    "    return np.ones(len(df), dtype=int)\n",
    "\n",
    "def policy_never(df):\n",
    "    \"\"\"Extreme policy: Never give contrast (0% contrast use).\"\"\"\n",
    "    return np.zeros(len(df), dtype=int)\n",
    "\n",
    "def policy_egfr_rule_30(df):\n",
    "    \"\"\"Toy rule: Withhold contrast if eGFR < 30 (egfr_cat == 0), otherwise give.\"\"\"\n",
    "    return (df['egfr_cat'] != 0).astype(int).values\n",
    "\n",
    "def policy_egfr_rule_45(df):\n",
    "    \"\"\"Toy rule: Withhold contrast if eGFR < 45 (egfr_cat in {0,1}), otherwise give.\"\"\"\n",
    "    return (~df['egfr_cat'].isin([0, 1])).astype(int).values\n",
    "\n",
    "policies = {\n",
    "    'Current Practice': policy_current,\n",
    "    'Always Contrast (100%)': policy_always,\n",
    "    'Never Contrast (0%)': policy_never,\n",
    "    'eGFR Rule: withhold if <30': policy_egfr_rule_30,\n",
    "    'eGFR Rule: withhold if <45': policy_egfr_rule_45,\n",
    "}\n",
    "\n",
    "\n",
    "# 2. Execution Loop\n",
    "# We evaluate on the primary outcome (AKI_30)\n",
    "outcome_name = 'AKI_30'\n",
    "col_date = f\"date_{outcome_name}\"\n",
    "print(f\"Evaluating policies for outcome: {outcome_name}\")\n",
    "\n",
    "# Prepare Arrays\n",
    "T_vec = df_final['contrast_received'].values\n",
    "# Define Y (Binary outcome within 30 days)\n",
    "Y_vec = ((df_final[col_date] - df_final['index_date']).dt.days <= 30).astype(int).values\n",
    "\n",
    "# Step A: Get Nuisance Parameters (Mu0, Mu1, Pi) via Cross-Fitting\n",
    "# We use the X_final sparse matrix\n",
    "stats, preds = run_cross_fitted_aipw(X_final, T_vec, Y_vec, n_folds=5)\n",
    "\n",
    "mu1 = preds['mu1']\n",
    "mu0 = preds['mu0']\n",
    "pi  = preds['pi']\n",
    "\n",
    "# Step B: Evaluate Policies\n",
    "policy_results = []\n",
    "\n",
    "for name, func in policies.items():\n",
    "    d_vec = func(df_final) # 0/1 vector\n",
    "    \n",
    "    # Calculate Policy Value V(d) using AIPW (Doubly Robust) estimator adapted for policies\n",
    "    # V_dr(d) = Mean( d/pi * (Y - mu1) + mu1 ) if d=1, ... logic generalizes:\n",
    "    # Estimate Y(1) for everyone, Y(0) for everyone, then mix based on d.\n",
    "    # DR Score for individual i:\n",
    "    # Gamma_1 = mu1 + T/pi * (Y - mu1)\n",
    "    # Gamma_0 = mu0 + (1-T)/(1-pi) * (Y - mu0)\n",
    "    # V(d) = d * Gamma_1 + (1-d) * Gamma_0\n",
    "    \n",
    "    gamma_1 = mu1 + (T_vec / pi) * (Y_vec - mu1)\n",
    "    gamma_0 = mu0 + ((1 - T_vec) / (1 - pi)) * (Y_vec - mu0)\n",
    "    \n",
    "    # Individual policy estimates\n",
    "    psi_i = d_vec * gamma_1 + (1 - d_vec) * gamma_0\n",
    "    \n",
    "    risk_val = np.mean(psi_i)\n",
    "    risk_se  = np.std(psi_i) / np.sqrt(len(psi_i))\n",
    "    \n",
    "    # Withholding Rate (W)\n",
    "    withhold_rate = np.mean(1 - d_vec)\n",
    "    \n",
    "    policy_results.append({\n",
    "        'Policy': name,\n",
    "        'Risk': risk_val,\n",
    "        'Risk_SE': risk_se,\n",
    "        'Withholding': withhold_rate\n",
    "    })\n",
    "\n",
    "df_pol_res = pd.DataFrame(policy_results)\n",
    "print(df_pol_res.round(5))\n",
    "\n",
    "# 3. Visualization (XY Plot)\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot points with error bars\n",
    "for i, row in df_pol_res.iterrows():\n",
    "    plt.errorbar(\n",
    "        x=row['Withholding'],\n",
    "        y=row['Risk'],\n",
    "        yerr=1.96 * row['Risk_SE'],\n",
    "        fmt='o',\n",
    "        markersize=10,\n",
    "        capsize=5,\n",
    "        label=row['Policy']\n",
    "    )\n",
    "    # Label text offset\n",
    "    plt.text(row['Withholding'], row['Risk'] + 0.0005, f\"  {row['Policy']}\", fontsize=9)\n",
    "\n",
    "# Formatting\n",
    "plt.title(f\"Policy Frontier: Harm vs. Withholding\\nOutcome: {outcome_name} (30-Day Risk)\")\n",
    "plt.xlabel(\"Proportion of Patients Withheld Contrast\")\n",
    "plt.ylabel(f\"Estimated Risk of {outcome_name}\")\n",
    "plt.grid(True, linestyle=':', alpha=0.6)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print AIPW stats for the contrast vs no-contrast comparison\n",
    "print(f\"ATE (Contrast vs No-Contrast): {stats['ATE']:.4f} \"\n",
    "      f\"[{stats['CI_Lower']:.4f}, {stats['CI_Upper']:.4f}], p={stats['P_Value']:.3g}\")\n",
    "print(f\"Risk_1 (E[Y | do(T=1)]): {stats['Risk_1']:.4f}\")\n",
    "print(f\"Risk_0 (E[Y | do(T=0)]): {stats['Risk_0']:.4f}\")\n",
    "print(f\"RR (Contrast vs No-Contrast): {stats['RR']:.3f}\")\n",
    "print(f\"E-Value (Contrast vs No-Contrast): {stats['E_Value']:.2f}\")\n",
    "print(f\"AIPW Effective Sample Size (AIPW weights): {stats['ESS']:.1f}\")\n",
    "\n",
    "# Current-practice policy value from the policy frontier\n",
    "cp_row = df_pol_res[df_pol_res['Policy'] == 'Current Practice'].iloc[0]\n",
    "print(f\"Current Practice Policy Risk (DR): {cp_row['Risk']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 11: SURVIVAL PLOTS ---\n",
    "\n",
    "outcome = 'AKI_30'\n",
    "print(f\"Diagnostic Plots for {outcome} (Observed Trial)...\")\n",
    "\n",
    "df_viz = df_final.copy()\n",
    "\n",
    "# Time-to-event for AKI within 30 days\n",
    "event_date = df_viz['date_AKI_30']\n",
    "idx_date = df_viz['index_date']\n",
    "\n",
    "# If no event, censor at 30 days\n",
    "event_date_filled = event_date.fillna(idx_date + pd.Timedelta(days=30))\n",
    "t_days = (event_date_filled - idx_date).dt.days\n",
    "t_days = t_days.clip(lower=0, upper=30)\n",
    "\n",
    "df_viz['T_viz'] = t_days\n",
    "df_viz['E_viz'] = ((event_date.notnull()) &\n",
    "                   ((event_date - idx_date).dt.days <= 30)).astype(int)\n",
    "\n",
    "# 1. KM Curves with IPTW\n",
    "kmf0 = KaplanMeierFitter()\n",
    "kmf1 = KaplanMeierFitter()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "kmf0.fit(\n",
    "    df_viz[df_viz['contrast_received'] == 0]['T_viz'],\n",
    "    df_viz[df_viz['contrast_received'] == 0]['E_viz'],\n",
    "    weights=df_viz[df_viz['contrast_received'] == 0]['iptw'],\n",
    "    label='Withheld'\n",
    ")\n",
    "kmf1.fit(\n",
    "    df_viz[df_viz['contrast_received'] == 1]['T_viz'],\n",
    "    df_viz[df_viz['contrast_received'] == 1]['E_viz'],\n",
    "    weights=df_viz[df_viz['contrast_received'] == 1]['iptw'],\n",
    "    label='Contrast'\n",
    ")\n",
    "\n",
    "kmf0.plot_survival_function()\n",
    "kmf1.plot_survival_function()\n",
    "plt.title(f\"Adjusted Survival Curve: {outcome}\")\n",
    "plt.xlabel(\"Days since index\")\n",
    "plt.ylabel(\"Survival probability (no AKI)\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Cox model & PH diagnostics\n",
    "cph = CoxPHFitter(penalizer=0.1)\n",
    "cph.fit(\n",
    "    df_viz[['T_viz', 'E_viz', 'contrast_received', 'iptw']],\n",
    "    duration_col='T_viz',\n",
    "    event_col='E_viz',\n",
    "    weights_col='iptw'\n",
    ")\n",
    "cph.check_assumptions(\n",
    "    df_viz[['T_viz', 'E_viz', 'contrast_received', 'iptw']],\n",
    "    show_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd398410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc3006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac05db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can also add subgroups n ts\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n--- CLINICAL USE SUBGROUP ANALYSIS (ITE-BASED) ---\")\n",
    "\n",
    "# 1. Safety & Globals Check\n",
    "required_globals = [\"df_final\", \"preds\", \"T_vec\", \"Y_vec\"]\n",
    "for name in required_globals:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Required object '{name}' not found. Run main AIPW pipeline first.\")\n",
    "\n",
    "if \"procedure_concept_id\" not in df_final.columns:\n",
    "    raise RuntimeError(\"Column 'procedure_concept_id' not found in df_final.\")\n",
    "\n",
    "# 2. Define Clinical Use Groups (Aligned with the Filtered Cohort)\n",
    "# These match the 'REVISED' filtering cell exactly.\n",
    "clinical_groups_map = {\n",
    "    \"Routine Abdominal/Pelvic\": {\n",
    "        # Combined\n",
    "        21492176, 3047782, 4335400, 37109313, 3049940,\n",
    "        # Abdomen Only\n",
    "        4139745, 36713200, 3018999,\n",
    "        # Pelvis Only\n",
    "        4252907, 3019625, 40771605, 36713202, 3035568\n",
    "    },\n",
    "    \"Routine Chest (Thorax)\": {\n",
    "        4327032, 3013610, 37117305, 3047921\n",
    "    },\n",
    "    \"Angiography (CTA/MRA - Vascular)\": {\n",
    "        3053128, 36713226, 45765683, 3053040\n",
    "    },\n",
    "    \"Neuro / Head (Brain)\": {\n",
    "        4197203, 36717294, 36713262, 36713243, 3024397\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Thresholds for Validity\n",
    "# We keep these from your previous cell to ensure we don't make claims on insufficient data\n",
    "min_total_N = 150\n",
    "min_events_per_arm = 5\n",
    "min_non_events_per_arm = 10\n",
    "\n",
    "print(\n",
    "    f\"Thresholds: N >= {min_total_N}, \"\n",
    "    f\"events per arm >= {min_events_per_arm}, \"\n",
    "    f\"non-events per arm >= {min_non_events_per_arm}\"\n",
    ")\n",
    "\n",
    "# 4. Calculate Individual Treatment Effects (ITE)\n",
    "# Gamma = Doubly Robust Score\n",
    "# We calculate Risk Difference (RD) = Risk_Contrast - Risk_NoContrast\n",
    "# Positive RD = Contrast increases Risk (Harm)\n",
    "# Negative RD = Contrast decreases Risk (Benefit)\n",
    "gamma_1_all = preds[\"mu1\"] + (T_vec / preds[\"pi\"]) * (Y_vec - preds[\"mu1\"])\n",
    "gamma_0_all = preds[\"mu0\"] + ((1 - T_vec) / (1 - preds[\"pi\"])) * (Y_vec - preds[\"mu0\"])\n",
    "delta_all   = gamma_1_all - gamma_0_all \n",
    "\n",
    "subgroup_results = []\n",
    "\n",
    "# 5. Execution Loop\n",
    "for group_label, codes in clinical_groups_map.items():\n",
    "    # Identify patients\n",
    "    mask = df_final['procedure_concept_id'].isin(codes)\n",
    "    N_sub = mask.sum()\n",
    "    \n",
    "    # Threshold Check 1: Total N\n",
    "    if N_sub < min_total_N:\n",
    "        print(f\"Skipping {group_label}: N={N_sub} (<{min_total_N})\")\n",
    "        continue\n",
    "\n",
    "    # Get Subgroup Data vectors\n",
    "    y_sub = Y_vec[mask]\n",
    "    t_sub = T_vec[mask]\n",
    "\n",
    "    # Threshold Check 2: Event Counts per Arm\n",
    "    # We need events in BOTH arms to calculate a valid RR\n",
    "    events_t1 = y_sub[t_sub==1].sum()\n",
    "    events_t0 = y_sub[t_sub==0].sum()\n",
    "    non_events_t1 = (1-y_sub[t_sub==1]).sum()\n",
    "    non_events_t0 = (1-y_sub[t_sub==0]).sum()\n",
    "\n",
    "    if (events_t1 < min_events_per_arm) or (events_t0 < min_events_per_arm):\n",
    "        print(f\"Skipping {group_label}: Insufficient events (T1={events_t1}, T0={events_t0})\")\n",
    "        continue\n",
    "        \n",
    "    if (non_events_t1 < min_non_events_per_arm) or (non_events_t0 < min_non_events_per_arm):\n",
    "        print(f\"Skipping {group_label}: Insufficient non-events.\")\n",
    "        continue\n",
    "\n",
    "    # Subset the ITE Scores (No Refit)\n",
    "    gamma1_sub = gamma_1_all[mask]\n",
    "    gamma0_sub = gamma_0_all[mask]\n",
    "    delta_sub  = delta_all[mask]\n",
    "    \n",
    "    # Statistics\n",
    "    risk1 = np.mean(gamma1_sub)\n",
    "    risk0 = np.mean(gamma0_sub)\n",
    "    ate   = np.mean(delta_sub) # Risk Difference\n",
    "    se    = np.std(delta_sub, ddof=1) / np.sqrt(N_sub)\n",
    "    \n",
    "    # Simple ESS proxy for this subgroup\n",
    "    p_treated = float(t_sub.mean())\n",
    "    ess_sub = 4.0 * N_sub * p_treated * (1.0 - p_treated)\n",
    "    \n",
    "    subgroup_results.append({\n",
    "        'Clinical Scenario': group_label,\n",
    "        'N': N_sub,\n",
    "        'Events_T1': events_t1,\n",
    "        'Events_T0': events_t0,\n",
    "        'Risk_Contrast': risk1,\n",
    "        'Risk_NoContrast': risk0,\n",
    "        'ATE': ate, # Risk Difference\n",
    "        'CI_Lower': ate - 1.96 * se,\n",
    "        'CI_Upper': ate + 1.96 * se,\n",
    "        'RR': risk1 / risk0 if risk0 > 0 else np.nan,\n",
    "        'ESS': ess_sub\n",
    "    })\n",
    "\n",
    "# 6. Display & Plot\n",
    "df_sub = pd.DataFrame(subgroup_results)\n",
    "\n",
    "if df_sub.empty:\n",
    "    print(\"\\nNo subgroups met the size/event thresholds.\")\n",
    "else:\n",
    "    print(\"\\n--- Results by Clinical Indication ---\")\n",
    "    cols = ['Clinical Scenario', 'N', 'Events_T1', 'Events_T0', 'ATE', 'RR', 'ESS']\n",
    "    print(df_sub[cols].round(4).to_string(index=False))\n",
    "\n",
    "    # Forest Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_plot = df_sub.sort_values('ATE')\n",
    "    y_pos = np.arange(len(df_plot))\n",
    "    \n",
    "    # Plot Error Bars\n",
    "    plt.errorbar(\n",
    "        x=df_plot['ATE'], \n",
    "        y=y_pos, \n",
    "        xerr=[\n",
    "            df_plot['ATE'] - df_plot['CI_Lower'], \n",
    "            df_plot['CI_Upper'] - df_plot['ATE']\n",
    "        ],\n",
    "        fmt='o', color='darkblue', ecolor='black', capsize=5, markersize=8,\n",
    "        label='ATE (Risk Difference)'\n",
    "    )\n",
    "    \n",
    "    # Aesthetics\n",
    "    plt.yticks(y_pos, [f\"{r['Clinical Scenario']}\\n(N={r['N']})\" for _, r in df_plot.iterrows()])\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='Null Effect (0)')\n",
    "    \n",
    "    # Title and Labels\n",
    "    plt.title(f\"Heterogeneity of Effect by Clinical Indication\\nOutcome: {outcome_name_pair if 'outcome_name_pair' in globals() else 'AKI_30'}\")\n",
    "    plt.xlabel(\"ATE (Risk Difference: Risk_Contrast - Risk_NoContrast)\\n<-- Contrast Beneficial (Protective) | Contrast Harmful (Risk Increase) -->\")\n",
    "    plt.grid(axis='x', linestyle=':', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425de29d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b179e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e8694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
